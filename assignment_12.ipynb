{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "# List files and folders\n",
    "!ls\n",
    "\n",
    "# Check the open jdk version on colab\n",
    "!ls /usr/lib/jvm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Apache Spark binary: This link can change based on the version. Update this link with the latest version before using\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# Unzip file\n",
    "!tar -xvzf spark-3.5.0-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q findspark\n",
    "\n",
    "# Install pyspark\n",
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
    "\n",
    "# findspark will locate spark in the system\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Hands-on PySpark on Google Colab\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set header=true as one of the options. This will read the first row as header\n",
    "spark_data = spark.read.format('csv').options(header='true').load(\"/content/sample_data/california_housing_train.csv\")\n",
    "spark_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Hardik Garg\\OneDrive\\Desktop\\assignment_12_q2.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hardik%20Garg/OneDrive/Desktop/assignment_12_q2.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import pyspark\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Hardik%20Garg/OneDrive/Desktop/assignment_12_q2.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hardik%20Garg/OneDrive/Desktop/assignment_12_q2.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m StructType, StructField, IntegerType, FloatType\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Hardik%20Garg/OneDrive/Desktop/assignment_12_q2.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39msum\u001b[39m, count, desc\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# QUESTION 2\n",
    "\n",
    "# Import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql.functions import sum, count, desc\n",
    "\n",
    "# Sample data\n",
    "data = [(101, 200.5), \n",
    "        (102, 100.2),\n",
    "        (103, 50.5), \n",
    "        (101, 75.3),\n",
    "        (102, 150.2)]\n",
    "\n",
    "# Schema \n",
    "schema = StructType([\n",
    "    StructField(\"User_ID\",IntegerType(),True),\n",
    "    StructField(\"Amount_Paid\",FloatType(),True)\n",
    "])\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"sample_data\").getOrCreate()\n",
    "\n",
    "# Create DataFrame \n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# Show sample DataFrame\n",
    "df.show()\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"grocery_analysis\").getOrCreate()\n",
    "\n",
    "# Assuming 'df' is the DataFrame created from your sample data\n",
    "\n",
    "# Group by 'User_ID', calculate total amount paid and count the number of visits\n",
    "result_df = df.groupBy(\"User_ID\").agg(\n",
    "    sum(\"Amount_Paid\").alias(\"Total_Amount_Paid\"),\n",
    "    count(\"User_ID\").alias(\"Number_of_Visits\")\n",
    ")\n",
    "\n",
    "# Sort the result in descending order based on the number of visits\n",
    "result_df = result_df.orderBy(desc(\"Number_of_Visits\"))\n",
    "\n",
    "# Show the final result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"data_operations\").getOrCreate()\n",
    "\n",
    "# Sample data for the first DataFrame\n",
    "data1 = [(\"Alice\", 25, \"Engineering\"),\n",
    "         (\"Bob\", 30, \"Finance\"),\n",
    "         (\"Charlie\", 22, \"Marketing\"),\n",
    "         (\"Alice\", 25, \"Engineering\"),  # Duplicate row\n",
    "         (\"David\", 28, \"Human Resources\")]\n",
    "\n",
    "# Schema for the first DataFrame\n",
    "schema1 = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the first DataFrame\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "\n",
    "# Show the number of columns and rows in the data\n",
    "print(\"1) Number of Columns and Rows:\")\n",
    "print(\"   Columns:\", len(df1.columns))\n",
    "print(\"   Rows:\", df1.count())\n",
    "\n",
    "# Remove duplicate rows\n",
    "df1 = df1.dropDuplicates()\n",
    "print(\"\\n2) DataFrame after removing duplicates:\")\n",
    "df1.show()\n",
    "\n",
    "# Crop any 3 columns and show them separately\n",
    "columns_to_crop = [\"Name\", \"Age\", \"Department\"]\n",
    "cropped_df1 = df1.select(columns_to_crop)\n",
    "print(\"\\n3) Cropped DataFrame:\")\n",
    "cropped_df1.show()\n",
    "\n",
    "# Create a second DataFrame for joining\n",
    "data2 = [(\"Alice\", \"Project Manager\"),\n",
    "         (\"Bob\", \"Financial Analyst\"),\n",
    "         (\"Charlie\", \"Marketing Specialist\"),\n",
    "         (\"David\", \"HR Manager\")]\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Job_Title\", StringType(), True)\n",
    "])\n",
    "\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "# Join the two DataFrames\n",
    "joined_df = df1.join(df2, \"Name\", \"inner\")\n",
    "print(\"\\n4) Joined DataFrame:\")\n",
    "joined_df.show()\n",
    "\n",
    "# Show graphs (example: Age distribution)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "age_distribution = df1.select(\"Age\").toPandas()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(age_distribution[\"Age\"], bins=20, kde=True)\n",
    "plt.title(\"Age Distribution\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
