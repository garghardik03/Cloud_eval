{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "\n",
    "!ls\n",
    "\n",
    "# Check the open jdk version on colab\n",
    "!ls /usr/lib/jvm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Apache Spark binary\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "\n",
    "# Unzip file\n",
    "!tar -xvzf spark-3.5.0-bin-hadoop3.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q findspark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Hands-on PySpark on Google Colab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data = spark.read.format('csv').options(header='true').load(\"/content/sample_data/california_housing_train.csv\")\n",
    "spark_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "from pyspark.sql.functions import sum, count, desc\n",
    "\n",
    "data = [(101, 200.5), (102, 100.2),(103, 50.5), (101, 75.3),(102, 150.2)]\n",
    "\n",
    "schema = StructType([StructField(\"User_ID\",IntegerType(),True),StructField(\"Amount_Paid\",FloatType(),True)])\n",
    "\n",
    "spark = SparkSession.builder.appName(\"sample_data\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"grocery_analysis\").getOrCreate()\n",
    "\n",
    "result_df = df.groupBy(\"User_ID\").agg(sum(\"Amount_Paid\").alias(\"Total_Amount_Paid\"),count(\"User_ID\").alias(\"Number_of_Visits\"))\n",
    "\n",
    "result_df = result_df.orderBy(desc(\"Number_of_Visits\"))\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data_operations\").getOrCreate()\n",
    "\n",
    "data1 = [(\"Alice\", 25, \"Engineering\"),(\"Bob\", 30, \"Finance\"),(\"Charlie\", 22, \"Marketing\"),(\"Alice\", 25, \"Engineering\"), (\"David\", 28, \"Human Resources\")]\n",
    "\n",
    "schema1 = StructType([StructField(\"Name\", StringType(), True),StructField(\"Age\", IntegerType(), True),\n",
    "                      StructField(\"Department\", StringType(), True)])\n",
    "\n",
    "df1 = spark.createDataFrame(data=data1, schema=schema1)\n",
    "\n",
    "print(\"1) Number of Columns and Rows:\")\n",
    "print(\"   Columns:\", len(df1.columns))\n",
    "print(\"   Rows:\", df1.count())\n",
    "\n",
    "df1 = df1.dropDuplicates()\n",
    "print(\"\\n2) DataFrame after removing duplicates:\")\n",
    "df1.show()\n",
    "\n",
    "columns_to_crop = [\"Name\", \"Age\"]\n",
    "cropped_df1 = df1.select(columns_to_crop)\n",
    "print(\"\\n3) Cropped DataFrame:\")\n",
    "cropped_df1.show()\n",
    "\n",
    "data2 = [(\"Alice\", \"Project Manager\"),(\"Bob\", \"Financial Analyst\"),(\"Charlie\", \"Marketing Specialist\"),(\"David\", \"HR Manager\")]\n",
    "\n",
    "schema2 = StructType([StructField(\"Name\", StringType(), True),StructField(\"Job_Title\", StringType(), True)])\n",
    "\n",
    "df2 = spark.createDataFrame(data=data2, schema=schema2)\n",
    "\n",
    "joined_df = df1.join(df2, \"Name\", \"inner\")\n",
    "print(\"\\n4) Joined DataFrame:\")\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "age_distribution = df1.select(\"Age\").toPandas()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(age_distribution[\"Age\"], bins=20, kde=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
